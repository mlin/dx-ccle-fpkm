#!/usr/bin/env python
# ccle_fetcher 0.0.1
# Generated by dx-app-wizard.
#
# Parallelized execution pattern: Your app will generate multiple jobs
# to perform some computation in parallel, followed by a final
# "postprocess" stage that will perform any additional computations as
# necessary.
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

MAX_SUBJOBS=8

import subprocess
import dxpy
import os
import xmltodict
import hashlib
import json
import time

@dxpy.entry_point("main")
def main(analysis_id):
    ccle_info_by_analysis_id, ccle_info_by_barcode = retry(get_ccle_index, 'cgquery')

    # validate each requested analysis id against the CCLE index
    for i in xrange(len(analysis_id)):
        if analysis_id[i] in ccle_info_by_barcode:
            analysis_id[i] = str(ccle_info_by_barcode[analysis_id[i]]['analysis_id'])
        if analysis_id[i] not in ccle_info_by_analysis_id:
            raise dxpy.AppError("Could not find {} in the CCLE index downloaded from CGHub. Ensure it's the analysis_id or legacy_sample_id/Barcode of a live CCLE dataset.".format(analysis_id[i]))
    if len(set(analysis_id)) < len(analysis_id):
        raise dxpy.AppError("The array of requested CCLE analysis IDs contains duplicates.")

    # Fetch all the files
    dxlinks = []
    for id in analysis_id:
        info = ccle_info_by_analysis_id[id]
        dxlinks = ccle_fetch_existing(info)
        if dxlinks is None:
            dxlinks = ccle_gtdownload(info)
        for dxlink in dxlinks:
            dxlinks.append(dxlink)

    return {'files': dxlinks}
    
    """
    # Split your work into parallel tasks.  As an example, the
    # following generates 10 subjobs running with the same dummy
    # input.

    subjobs = []
    for i in range(10):
        subjob_input = { "input1": True }
        subjobs.append(dxpy.new_dxjob(subjob_input, "process"))

    # The following line creates the job that will perform the
    # "postprocess" step of your app.  We've given it an input field
    # that is a list of job-based object references created from the
    # "process" jobs we just created.  Assuming those jobs have an
    # output field called "output", these values will be passed to the
    # "postprocess" job.  Because these values are not ready until the
    # "process" jobs finish, the "postprocess" job WILL NOT RUN until
    # all job-based object references have been resolved (i.e. the
    # jobs they reference have finished running).
    #
    # If you do not plan to have the "process" jobs create output that
    # the "postprocess" job will require, then you can explicitly list
    # the dependencies to wait for those jobs to finish by setting the
    # "depends_on" field to the list of subjobs to wait for (it
    # accepts either dxpy handlers or string IDs in the list).  We've
    # included this parameter in the line below as well for
    # completeness, though it is unnecessary if you are providing
    # job-based object references in the input that refer to the same
    # set of jobs.

    postprocess_job = dxpy.new_dxjob(fn_input={ "process_outputs": [subjob.get_output_ref("output") for subjob in subjobs] },
                                     fn_name="postprocess",
                                     depends_on=subjobs)

    # If you would like to include any of the output fields from the
    # postprocess_job as the output of your app, you should return it
    # here using a job-based object reference.  If the output field in
    # the postprocess function is called "answer", you can pass that
    # on here as follows:
    #
    # return { "app_output_field": postprocess_job.get_output_ref("answer"), ...}
    #
    # Tip: you can include in your output at this point any open
    # objects (such as gtables) which will be closed by a job that
    # finishes later.  The system will check to make sure that the
    # output object is closed and will attempt to clone it out as
    # output into the parent container only after all subjobs have
    # finished.
    """

"""

@dxpy.entry_point("process")
def process(input1):
    # Change the following to process whatever input this stage
    # receives.  You may also want to copy and paste the logic to download
    # and upload files here as well if this stage receives file input
    # and/or makes file output.

    print input1

    return { "output": "placeholder value" }

@dxpy.entry_point("postprocess")
def postprocess(process_outputs):
    # Change the following to process whatever input this stage
    # receives.  You may also want to copy and paste the logic to download
    # and upload files here as well if this stage receives file input
    # and/or makes file output.

    for output in process_outputs:
        pass

    return { "answer": "placeholder value" }

"""

# Given the metadata of a CCLE dataset from the index, see if all its
# constituent files are already present in the project. If so, return a list
# of DXLinks to them; otherwise return None.
def ccle_fetch_existing(info):
    analysis_id = str(info['analysis_id'])
    expected_files = ccle_expected_files(info)
    print '\n\nLooking for existing data for {} in the project, consisting of files: {}'.format(analysis_id,json.dumps(expected_files))

    # for each expected file, see if it's already in the project
    existing = []
    for md5 in expected_files:
        for candidate in dxpy.find_data_objects(project=dxpy.PROJECT_CONTEXT_ID,
                                                classname='file',
                                                state='closed',
                                                name=expected_files[md5],
                                                name_mode='exact',
                                                properties={'md5': md5},
                                                return_handler=True):
            deets = candidate.get_details()
            if 'cghub_metadata' in deets and 'md5' in deets and deets['md5'] == md5:
                existing.push(candidate)
                break

    # if the project already has all of them, we can quit early
    if len(existing) == len(expected_files):
        print 'The files are already in the project!'
        ids = [dxfile.get_dxid() for dxfile in existing]
        DXProject(dxpy.PROJECT_CONTEXT_ID).clone(dxpy.WORKSPACE_ID,objects=ids)
        return [dxpy.dxlink(id) for id in ids]
    elif len(existing) > 0:
        print 'Only some of the files are already in the project!'
    else:
        print 'No existing data found in the project.'

    return None

# gtdownload and import one CCLE dataset, given its metadata entry from the
# index. Return a list of DXLinks
_installed_genetorrent=False
def ccle_gtdownload(info):
    analysis_id = str(info['analysis_id'])
    expected_files = ccle_expected_files(info)
    print 'Downloading {}, consisting of files: {}'.format(analysis_id,json.dumps(expected_files))

    # perform the download to local scratch space
    global _installed_genetorrent
    if not _installed_genetorrent:
        sh("dpkg -i genetorrent-common.deb")
        sh("dpkg -i genetorrent-download.deb")
        _installed_genetorrent = True
    try:
        sh("gtdownload -c https://cghub.ucsc.edu/software/downloads/cghub_public.key -v -d {}".format(analysis_id))
    except:
        raise dxpy.AppError("Failed to download the data from CGHub using GeneTorrent. Ensure the analysis ID(s) are correct and check the job log for more details. Note: CGHub has scheduled maintenance windows on Tuesdays and Thursdays from 1:00-5:00 PM Pacific.")

    # validate the files gtdownload placed in the expected subdirectory
    if not os.path.isdir(analysis_id):
        raise dxpy.AppInternalError("Unexpected: GeneTorrent gtdownload did not create a subdirectory for " + analysis_id)
    print 'Verifying gtdownload products'
    products = []
    for dirname, subdirs, filenames in os.walk(analysis_id):
        if len(subdirs) > 0:
            raise dxpy.AppInternalError("Unexpected: GeneTorrent gtdownload created subdirectories " + subdirs)
        for filename in filenames:
            filepath = os.path.join(dirname, filename)
            md5 = md5sum(filepath)
            print "{} {}".format(md5,filename)
            if md5 not in expected_files:
                raise dxpy.AppInternalError("GeneTorrent gtdownload produced a file {} with MD5 {} which does not match a file in the CCLE index for {}".format(filename, md5, analysis_id))
            if str(filename) != str(expected_files[md5]):
                raise dxpy.AppInternalError("GeneTorrent gtdownload produced a file {} but the expected name was {} based on the CCLE index for {}".format(filename, expected_files[md5], analysis_id))
            del expected_files[md5]
            products.append((md5, filepath))

    # make sure we got everything
    if len(expected_files) > 0:
        raise dxpy.AppInternalError("GeneTorrent gtdownload did not produce all expected files for {}. Missing: {}".format(analysis_id, expected_files))

    # upload to platform
    print 'Uploading to platform'
    dxlinks = []
    for (md5,filename) in products:
        dxfile = dxpy.upload_local_file(filename,keep_open=True)

        # store some metadata
        dxfile.set_details({'md5': md5, 'cghub_metadata': info})
        dxfile.set_properties({'md5': md5})
        dxfile.add_tags(['from_ccle_fetcher'])

        # add file to the list of output files
        dxfile.close()
        dxlinks.append(dxpy.dxlink(dxfile.get_id()))

    return dxlinks

# Use cgquery to fetch the CCLE index from CGHub; return two dicts with the
# metadata, one indexed by analysis_id and the other by legacy_sample_id
# (aka Barcode)
def get_ccle_index():
    # use cgquery to download xml
    try:
        sh("./cgquery 'state=live&study=*Other_Sequencing_Multiisolate' -o ccle.xml")
    except:
        raise dxpy.AppError("Failed to download CCLE index from CGHub using cgquery. Check the job log for more details. Note: CGHub has scheduled maintenance windows on Tuesdays and Thursdays from 1:00-5:00 PM Pacific.")
    try:
        # parse xml
        results = xmltodict.parse(open('ccle.xml'))['ResultSet']['Result']

        # index results
        results_by_analysis_id = {}
        results_by_barcode = {}
        for result in results:
            results_by_analysis_id[str(result['analysis_id'])] = result
            results_by_barcode[str(result['legacy_sample_id'])] = result
        return results_by_analysis_id, results_by_barcode
    except:
        raise dxpy.AppError("Could not parse CCLE index XML downloaded from CGHub using cgquery. Note: CGHub has scheduled maintenance windows on Tuesdays and Thursdays from 1:00-5:00 PM Pacific.")

# given one entry from the CCLE index, make a (md5 -> filename) dict for each
# constituent file
def ccle_expected_files(info):
    analysis_id = str(info['analysis_id'])
    expected_files = {}
    for expected_file in info['files']['file']:
        if 'checksum' not in expected_file or '@type' not in expected_file['checksum'] or str(expected_file['checksum']['@type']).lower() != "md5" or "#text" not in expected_file['checksum'] or len(expected_file['checksum']['#text']) == 0:
            raise dxpy.AppInternalError("Could not extract file MD5 checksums from CGHub index metadata for {}".format(analysis_id))
        if 'filename' not in expected_file:
            raise dxpy.AppInternalError("Could not extract filename from CGHub index metadata for {}".format(analysis_id))
        expected_files[str(expected_file['checksum']["#text"])] = str(expected_file['filename'])
    return expected_files

def sh(cmd):
    subprocess.check_call(cmd, shell=True)

# http://stackoverflow.com/questions/1131220/get-md5-hash-of-big-files-in-python
def md5sum(filename):
    md5 = hashlib.md5()
    with open(filename,'rb') as f: 
        for chunk in iter(lambda: f.read(128*md5.block_size), b''): 
             md5.update(chunk)
    return md5.hexdigest()

def retry(f, caption='operation', retries=5, backoff=1):
    try:
        return f()
    except:
        if retries <= 0:
            raise
        else:
            time.sleep(backoff)
            print 'retry: {}'.format(caption)
            retry(f, caption, retries-1, backoff*2)

dxpy.run()
